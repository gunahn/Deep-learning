{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 521153S, Deep Learning Final Project: Mini Image classification Competition with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline \n",
    "### In this assignment, you will learn:\n",
    "* Combine all you learned from the previous assignments.\n",
    "* Build your own CNN as you like with Pytorch, train and validate it on a given dataset.\n",
    "* Test your CNN model on our server.\n",
    "\n",
    "### Tasks (<span style=\"color:orange\">40 points</span>)\n",
    "We want to keep the task as clean and simple as possible. \n",
    "1. You would be given a dataset containing 45,000 grayscale 32x32 images. Also, we will set a private testing data and upload it to our server. You could only use the private testing data to test your model. Instructions about evaluating your model would be given along with the project in Moodle.\n",
    "2. Before evaluating your model on the server, you have to train your model based on the given 45,000 images. Specifically, in these images, there are nine classes, and each class has 5000 images. The testing data on the server has 9000 images, and each class has 1000 images.\n",
    "3. To get a good CNN model, basically, there are some rules for you to follow which would be considered when grading your report:\n",
    "  * Make the best out of the given images. It means you have to split it into training and validation set, training your model on the training set and validating the trained model on the validation set. If the accuracy on the validation set is not good, then you have to adjust your CNN model structure or some adjustable parameters. For example, batch size, learning rate, momentum on SGD, lambda in weight decay, etc.  Then retrain your model until the accuracy on the validation set is good enough because the testing data would have similar accuracy with the validation set. \n",
    "  * It also means that you could do some augmentation on the training set. This includes randomly flipping, cropping a small window in a random location within the image(refer to assignment 4), adding some noise, resizing-and-cropping, etc. All these are to make the training process more tolerant. \n",
    "  * From the CNN structure perspective, you also need to design your CNN model by yourself. Well-known network architecture you can use includes [ResNet](https://arxiv.org/abs/1512.03385), [Inception](https://arxiv.org/abs/1512.00567), [VGGNet](https://arxiv.org/abs/1409.1556), [DenseNet](https://arxiv.org/abs/1608.06993), [MobileNet](https://arxiv.org/abs/1801.04381), [ShuffleNet](https://arxiv.org/abs/1807.11164), [ResNeXt](https://arxiv.org/abs/1611.05431) etc. \n",
    "4. Similar to real-life applications, your model will be tested with unknown data. In this project,  after training and validating the model, you need to test it on our hold-on testing dataset. We will provide you with a submission server and a leaderboard. The instructions would be given alongside the project in Moodle. \n",
    "5. Please give a pdf report (also your source code, e.g., this Jupyter notebook file), documenting the whole model training process and also the evaluated accuracy on the server. Tensorboard visualization is also necessary for your report to visualize your network structure, accuracy, and losses, etc. as done in assignment 4.\n",
    "6. You need to return the pdf report as well as your trained model (a checkpoint file) with your source code file to moodle. We will run your model on the server and compare the results with the one written in the reports. \n",
    "\n",
    "### Grading\n",
    "You can get 40 points in total.\n",
    "  * You will get <span style=\"color:orange;font-weight:bold\">20 points</span> if your model achieve more than 82.5% of the testing accuracy.\n",
    "  * You will get <span style=\"color:orange;font-weight:bold\">20 points</span> if your report is clear and well-organized.\n",
    "  \n",
    "### Files you have to submit\n",
    "please submit a .zip file containing:\n",
    "1. a pdf report;\n",
    "2. source code files (jupyter notebook or common python files);\n",
    "3. a checkpoint file (which saves your trained model).\n",
    "\n",
    "### Group members\n",
    "Maximum 2 members. \n",
    "\n",
    "\n",
    "### Environment\n",
    "Python 3, Numpy, matplotlib, torch, torchvision...\n",
    "\n",
    "### Dataset\n",
    "Please follow the code below to download the 45,000 images and corresponding labels. <br>\n",
    "We have already split them into training and validation set; please refer to assignment 3 and 4 to create your DataLoader, with your data augmentation methods. Good luck. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading https://files.pythonhosted.org/packages/66/cc/276bcc98fb2d1e609c6c2230cc9ad76a3a29839f79c91e608cfb347d6ad7/utils-1.0.0-py2.py3-none-any.whl\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was already downloaded and extracted!\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import os, time\n",
    "import torch \n",
    "import requests, zipfile, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import download_given_data, get_preds_figure\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils\n",
    "import random, matplotlib\n",
    "import pandas as pd\n",
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "download_given_data('./')\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "### GROUP INFO\n",
    "### GROUP NAME: maksalaatikko\n",
    "### ID: 004\n",
    "### PWD: 2497530\n",
    "### IMAGE LIST: 004\n",
    "\n",
    "### IF YOU DON'T WANT TO USE GPU FOR THIS PROJECT, TOGGLE THE BOOL BELOW\n",
    "use_cuda = True\n",
    "#use_cuda = use_cuda and torch.cuda.is_available()\n",
    "\n",
    "print(use_cuda)\n",
    "###HYPERPARAMETERS\n",
    "batch_size = 15\n",
    "num_epochs = 20\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "#Download the model from here\n",
    "#https://unioulu-my.sharepoint.com/:u:/g/personal/tmusta_student_oulu_fi/EZrVZjNykZBAigx2d84xtPIBQdTNAYJXrAj2qGmoRsyWMw\n",
    "\n",
    "### Whether to train a new model or to test an old one. Old model requires 'model.weights' to exist in $PWD\n",
    "train_model = True\n",
    "test_model = True\n",
    "save_path = 'model.weights'\n",
    "if not train_model:\n",
    "    assert os.path.isfile(save_path), \"No model to be tested\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'group_004_test.csv' does not exist: b'group_004_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ca56db5568c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'group_004_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvaltransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-ca56db5568c9>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, imagefile, transform)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mimage_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagefile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'group_004_test.csv' does not exist: b'group_004_test.csv'"
     ]
    }
   ],
   "source": [
    "###DATASET. ACCEPTS CSV FILE AS ARGUMENT. CSV FILE SHOULD CONTAIN IMAGE PATH AND ACCORDING ANNOTATION WITH IT.\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imagefile, transform = None):\n",
    "        self.classes = []\n",
    "        self.samples = []\n",
    "        \n",
    "        image_list = pd.read_csv(imagefile, header=None)\n",
    "        self.samples = [image_list.iloc[i][0] for i in range(len(image_list))]\n",
    "        self.classes = [image_list.iloc[i][1] for i in range(len(image_list))]\n",
    "            \n",
    "        self.transform = transform\n",
    "        self.classes_n = max(self.classes)+1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, ndx):\n",
    "        sample = self.samples[ndx]\n",
    "        label = None\n",
    "        if self.classes[ndx] < 0:\n",
    "            label = self.classes[ndx]\n",
    "        else:\n",
    "            label = torch.zeros((self.classes_n, 1))\n",
    "            label[self.classes[ndx]] = 1\n",
    "            label = torch.t(label).squeeze()\n",
    "        sample = Image.open(sample)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n",
    "    \n",
    "    def get_classes_n(self):\n",
    "        return self.classes_n\n",
    "\n",
    "traintransform = transforms.Compose([\n",
    "                                transforms.Grayscale(num_output_channels=3),\n",
    "                                #transforms.Resize(256)\n",
    "                                transforms.Resize(224),\n",
    "                                #transforms.CenterCrop(224),\n",
    "                                #transforms.RandomRotation(degrees=90),\n",
    "                                transforms.RandomHorizontalFlip(0.25),\n",
    "                                #transforms.RandomVerticalFlip(0.25),\n",
    "                                transforms.ToTensor()\n",
    "                                #transforms.RandomErasing()\n",
    "                                #transforms.Normalize([0.5],[0.5])\n",
    "                                    ])\n",
    "valtransform = transforms.Compose([\n",
    "                              transforms.Grayscale(num_output_channels=3),\n",
    "                              transforms.Resize(224),\n",
    "                              transforms.ToTensor()])\n",
    "trainset = ImageDataset('given_data/train.csv', transform=traintransform)  \n",
    "valset = ImageDataset('given_data/val.csv', transform=valtransform)\n",
    "testset = None\n",
    "if test_model:\n",
    "    testset = ImageDataset('group_004_test.csv', transform=valtransform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, num_workers=64, shuffle=False)\n",
    "testloader = None\n",
    "if test_model:\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, num_workers=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL\n",
    "\n",
    "classes_n = trainset.get_classes_n()\n",
    "model = ResNet(BasicBlock, [2,2,2,1])\n",
    "\n",
    "n_inputs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(n_inputs, classes_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VALIDATION\n",
    "def validate(loader, m, criterion):\n",
    "    m.eval()\n",
    "    correct = 0.0\n",
    "    overall = 0.0\n",
    "    loss = 0.0\n",
    "    for i, data in enumerate(loader):\n",
    "        sample, label = data\n",
    "        if use_cuda:\n",
    "            sample = sample.cuda()\n",
    "            label = label.cuda()\n",
    "            \n",
    "        output = m(sample)\n",
    "        loss += criterion(output, label).item()\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        label = torch.argmax(label, dim=1)\n",
    "        correct += sum([int(i == j) for i,j in zip(output, label)])\n",
    "        overall += len(output)\n",
    "    return correct / overall, loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING\n",
    "def train(loader, model, log='tb_graphs'):\n",
    "    writer = SummaryWriter(log+'/training')\n",
    "    val_writer = SummaryWriter(log+'/validation')\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    best_acc = 0.0\n",
    "    iters = len(loader)\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    log_every = int(iters/ 10)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, data in enumerate(loader):\n",
    "        \n",
    "            sample, label = data\n",
    "            if use_cuda:\n",
    "                sample = sample.cuda()\n",
    "                label = label.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(sample)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            output = torch.argmax(output, dim=1)\n",
    "            label = torch.argmax(label, dim=1)\n",
    "            train_acc += sum([int(i == j) for i,j in zip(output, label)]) / batch_size\n",
    "            train_loss += loss.item()\n",
    "            if not i % log_every :\n",
    "                print(\"Training\", epoch, i, train_acc / log_every, train_loss / log_every)\n",
    "                log_index = epoch * iters + i\n",
    "                writer.add_scalar('Loss', train_loss / log_every, log_index) \n",
    "                writer.add_scalar('Accuracy', train_acc / log_every, log_index)\n",
    "\n",
    "                \n",
    "                train_loss = 0.0\n",
    "                train_acc = 0.0\n",
    "        acc, val_loss = validate(valloader, model, criterion)\n",
    "        val_writer.add_scalar('Loss', val_loss, log_index) \n",
    "        val_writer.add_scalar('Accuracy', acc, log_index)\n",
    "        print(\"Validation\", acc)\n",
    "        if acc > best_acc:\n",
    "            \n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        elif epoch > num_epochs / 2:\n",
    "        \n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST\n",
    "def test(loader, m, filename):\n",
    "    preds = []\n",
    "    m.eval()\n",
    "    for i, data in enumerate(loader):\n",
    "        sample, label = data\n",
    "        if use_cuda:\n",
    "            sample = sample.cuda()\n",
    "            \n",
    "        output = m(sample)\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        for i in output:\n",
    "            preds.append(int(i.cpu()))\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in preds:\n",
    "            f.write(\"%s\\n\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN PROGRAM\n",
    "\n",
    "if train_model:\n",
    "    logdir = 'resnet2221_hflipping_smoothl1_'+str(batch_size)+'_'+str(num_epochs)\n",
    "    train(trainloader, model, log=logdir)\n",
    "\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "print(\"Accuracy of model: \", validate(valloader, model, nn.SmoothL1Loss())[0])\n",
    "if test_model:\n",
    "    test(testloader, model, 'predictions.txt')\n",
    "    print(\"Test results created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
